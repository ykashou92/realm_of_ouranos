---
title: Modelling an ANN with Spindle Neurons
output:
    html_document:
        toc: true
        toc_float: true
        fig_caption: true
---

***

## Spindle Neurons in Biology

__Spindle neurons__ (SNs) or __von Economo neurons__ (VENs) differ from ordinary neurons in a couple of ways:

- __Structure__ : They are structurally different, and are usually much longer that tapers into a single _axon_ (output) and have only a single _dentrite_ (input) as opposed to ordinary neurons which usually have many _dendtrites_.

- __Abundance__ : They are present in specific regions of large mammalian brains and only a few recorded species have them, they represent an adaptation to larger brains, especially for the function of transmitting information faster.

***

## Proposed Mathematics of an Artificial Spindle Neuron

In machine learning, a neuron, or a classifier unit is modelled by having multiple inputs and a single output - similar to neurons in biology.

Likewise, we are going to attempt to create a neuron, that has a single input and a single output - similar to spindle neurons in biology, however it will also carry it's characteristic fast information transfer with lower information entropy.

__Artificial Neuron__

$$
p(y = 1 | x) = \sigma(w^{T} X)
$$

__Artificial Spindle Neuron__ 

$$
\psi (\sigma(w^{T} X), \ \beta, \ \delta) = \sigma \cdot\ln{(\delta \beta)}
$$

Where:  

- $\psi$ represents the spindle neuron function  

- $\sigma(z)$ represents the output from a neuron, specifically $p(y = 1 | x)$, i.e. the _probability_ of $y = 1$ _given_ $x$  

- $\delta$ represents the _difference in momentum_, or _required velocity of information transfer_, i.e. it is a function of _inverse entropy_ $(\epsilon^{-1})$.  
  
In other words, we must transform the information $\sigma$ that we wish to transfer by the inverse entropy function $\delta$, such that the information that $\sigma$ holds remains relevant to the network as a whole, which we will keep track of using a parameter $\beta$, as a measure of relevancy decay.

$$
\beta = \alpha \epsilon^{-1}
$$

Where:  

- $\beta$ = decayed relevancy  

- $\alpha$ = learning rate  

- $\epsilon$ = information entropy  


$$
\Delta = \delta\beta = \delta \alpha \epsilon^{-1} 
$$


Where:  

- $\delta$ is the relative discrete distance from neuron A to neuron B  

- $\delta$, multiplied by $\beta$ $(\alpha \epsilon^{-1})$.  

$$
\psi (\sigma(z), \ \beta, \ \delta) = |\sigma(w^T X)| \cdot \ln{(\delta \alpha \epsilon^{-1})}
$$


```{r}
psi_function = function(alpha, epsilon, delta, sigma) {
    psi = abs(sigma)*log2(delta*(alpha)*(epsilon^-1))
    return(psi)
}
psi_function(alpha = c(0.01, 0.1, 1.0, 10.0, 100), # learning rates
             epsilon = 0.30, # information entropy, % of information loss
             delta = 7, # 7 Networks to pass through
             sigma = 0.75)# probability
```

***

### What does the value of __$\psi$__ actually _mean_? 

__Objectives__ :   
Where $\psi \in \mathbb{Z}$... 

1. Explain the meaning of the value of $\psi \longrightarrow \infty$  
2. Explain the meaning of the value of $\psi \longrightarrow -\infty$  
3. What does each of the values in $\psi = \{-1, 0, 1\}$ mean?  

***
